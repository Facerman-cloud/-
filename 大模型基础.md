<h1 id="CvlEI">一、语言模型基础</h1>
<h2 id="esXZP">1.1、基于统计方法的语言模型</h2>
语言模型通过对语料库（Corpus）中的语料进行统计或学习来获得预测语言符号概率的能力。通常，基于统计的语言模型通过直接统计语言符号在语料库中出现的频率来预测语言符号的概率。其中，n-grams是最具代表性的统计语言模型。n-grams语言模型基于马尔可夫假设和离散变量的极大似然估计给出语言符号的概率。

<h3 id="RQLDN">1.1.1、n-grams语言模型</h3>
设包含 _N_个元素的语言符号可以表示为 _w_<sub>1:</sub><sub>_N_</sub>= {_w_<sub>1</sub>_, w_<sub>2</sub>_, w_<sub>3</sub>_, ..., w_<sub>_N_</sub>}。_w_<sub>1:</sub><sub>_N_</sub>可以代表文本，也可以代表音频序列等载有语义信息的序列。

n-grams语言模型中的 n-gram指的是长度为 _n_的词序列。n-grams语言模型通过依次统计文本中的 n-gram及其对应的 (n-1)-gram在语料库中出现的相对频率来计算文本 _w_<sub>1:</sub><sub>_N_</sub>出现的概率。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740015776803-4f14e21a-c87d-40f6-8d10-b3ba9cc3a18e.png)

n-grams语言模型中，n是变量，当n=1被称为unigram，其不考虑文本的上下文关系。当n=2时，称为bigrams，其对前一个词进行考虑。以此类推，当n=3时，称为trigrams，对前两个词进行考虑。例子如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740015949821-f7015406-2239-44f6-a23d-5aec0093f3bc.png)

在此例中，我们可以发现虽然“长颈鹿脖子长”并没有直接出现在语料库中，但是 bigrams语言模型仍可以预测出“长颈鹿脖子长”出现的概率有![image](https://cdn.nlark.com/yuque/__latex/fca9121757332bb7d8a6dcc4c4dceb92.svg)由此可

见，n-grams具备**对未知文本的泛化能力**。

而使用trigrams时：

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740016139475-ab1e1317-942d-4a10-852c-d013cc05266e.png)

n-grams中的n的选择会影响n-grams模型的泛化性能和计算复杂度。实际中的n通常小于等于5。

在n-grams语言模型中，**n代表了拟合语料库中的能力与对未知文本的泛化能力之间的权衡**。当n过大时，语料库中难以找到和n-gram一模一样的词序列，可能出现大量“零概率”现象；当n过小时，n-gram难以承载足够的语言信息，不足以反应语料库的特性

<h3 id="dDKvY">1.1.2、n-grams中的统计学原理</h3>
n-grams语言模型是在n阶马尔可夫假设下，对语料库中出现的**长度为n的词序列出现概率的极大似然估计，即使用n-grams模型最大概率的去生成当前的语料库。、**

<font style="color:#DF2A3F;">公式推导略</font>

**n-grams模型的缺点：**观测长度有限，无法捕捉长程依赖关系。此外，其是逐字匹配的，不能很好的适应语言的复杂性。 

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740017078728-b9ce7959-a4e7-4011-8409-13aec8d696c0.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740017112281-ef0e1408-271d-4603-a19a-dea5c635aff6.png)

<h2 id="HylvG">1.2、基于学习的语言模型</h2>
**统计与学习的区别：统计侧重于设计一个模型描摹已知；学习侧重于找到一个模型利用已知预测未知**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740101584325-a33277a6-37c7-4dca-8c68-736cbdec94ac.png)

机器学习的过程：在某种学习范式下，基于训练数据，利用学习算法，从受归纳偏置（限制对某些假设进行选择）限制的假设类（所有可能机器学习模型的集合）中选取可以达到学习目标的假设，该假设可以泛化到未知的数据上。

<h2 id="Wf60s">1.3、RNN与Transformer</h2>
将上下文输入给语言模型的两种方式：串行输入与并联输入。

<h3 id="e1fGC">1.3.1、RNN</h3>
典型的串行输入的语言模型是循环神经网络（Recurrent Neural Network，RNN）,其中一类网络连接中包含环路的神经网络的总称。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740103335509-b61b6434-3205-4509-b65d-ed684f57077a.png)

RNN可以将历史状态以隐变量的形式叠加到当前状态，对历史信息进行考虑，呈现出螺旋式前进的模式。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740103559576-9fa2a44a-3f54-43db-8b9c-4cc714ee21e0.png)

在训练RNN时，涉及大量的矩阵连乘的操作，容易引发梯度衰减或者梯度爆照的现象。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740103789315-a2c428f8-9040-4b9a-8fd7-032e099ba567.png)

为了解决经典RNN的梯度衰减/爆炸问题，带有门控机制的LSTM被提出。LSTM将经典RNN中的通过复合函数传递隐藏状态的方式，解耦为状态累加。隐藏状态通过遗忘门、输入门来实现合理的状态累加，通过输出门实现合理整合。

其中：遗忘门用以适度遗忘“往事”，输入门用以对输入信息进行选择性的“聆听”，输出门，将当前的状态适度输出。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740104014416-a997d2f8-d98c-45b9-a6b2-375a23f52c27.png)

GRU模型：为降低LSTM的计算成本，GRU将遗忘门与输入门进行合并。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740104861774-03611172-01e7-46be-96be-e5b53af07634.png)

<h3 id="AAYfC">1.3.2、Transformer</h3>
典型的支持**并行输入**的模型是Transformer，其是一类基于注意力机制（Attention）的模块化构建的神经网络结构。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740104963043-831c2a66-185c-4276-8ba3-736eea240ad2.png)

Transformer中两种主要模块为（1）注意力模块；（2）全连接前馈模块。其中，注意力模块负责对上下文进行通盘考虑。全连接前馈模块占据了Transformer近三分之二的参，掌管着Transformer模型的记忆。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740105145553-65a162ed-7a2d-40f5-abca-7209bdf3f70f.png)

<h4 id="DEa8M">1.3.2.1注意力模块</h4>
由自注意力层、残差连接和层正则化组成。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740105242642-fb30f13f-fbb4-4bf5-8e1e-85551d54ace1.png)

<h5 id="YPBBP">注意力层</h5>
采用加权平均的思想将前文信息叠加到当前的状态上。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740105353182-fb03ed85-afc3-418b-ae5e-263b331a7479.png)

<h5 id="q4xUF">层正则化与残差连接</h5>
层正则化用以加速神经网络训练过程并取得更好的泛化性能；引入残差连接可以有效解决梯度消失问题。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740106300897-825dcaa4-13e3-4435-a475-c5015566aef0.png)

<h3 id="hSpr4">1.3.3基于RNN和Transformer的语言模型</h3>
以预测下一词出现的概率为目标，解释基于RNN和Transformer的语言模型的训练流程以及推理过程。

训练下一词预测模型

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740106549774-dc9e009a-de56-47c9-9971-af37cc2f2d24.png)

<h3 id="AZiaR">1.3.4、自回归</h3>
将输出向量解码为对应的单词，然后“自回归”的不断生成新的词，最后组成一段文本。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740106656249-df6ac988-be0b-48e3-a361-3e7735045797.png)

但自回归存在如下问题：

+ 错误级联放大：选用模型自己生成的词作为输入可能会有错误，这样的错误循环输入，将会不断放大错误，导致模型不能很好的拟合训练集。
+ 串行效率低：因为下一个要预测的词依赖上一个预测的值，每次预测都是串行的，难以进行并行加速。

为解决上述为题，Teacher Forcing在语言模型预训练过程中被广泛应用。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740106939805-9743bd68-8e20-4764-b07a-42065d477491.png)

但是Teacher Forcing的训练方式将导致曝光偏差（Exposure Bias）:训练模型的过程和模型在推理过程中存在差异。其容易导致出现模型幻觉问题。

<h2 id="ITFmQ">1.4、语言模型的采样方法</h2>
语言模型每轮预测输出的是一个概率向量。我们需要根据概率值从词表中选出本轮输出的词元，选择词元的过程被称为采样。

两类主流的采样方法可以总结为（1）.概率最大化方法；（2）.随机采样方法

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740124354178-4c1a74ff-ffe3-4c46-9b17-575f4f6f6451.png)

<h3 id="cto7U">1.4.1、概率最大化方法</h3>
假设生成M个词元，概率最大化方法的搜索空间为M<sup>D</sup>，是NP-Hard问题（NP-Hard（NP困难）是计算复杂性理论中的一个重要概念，用于描述一类特别难解的问题。）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740124564479-ca133674-068c-4c39-95bf-f6e8c35f3649.png)

贪心搜索方法只关注“眼前利益”，忽略了“远期效益”。当前概率大的词有可能导致后续的词概率都很小。贪心搜索方法容易陷入局部最优，难以达到全局最优解。

波束搜索在每轮预测中考虑了更多的可能性，从而可以在一定程度上减少陷入局部最优的情况。达到全局最优的可能性更高。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740124791238-6b3d1f55-f6fa-4f98-83e2-53e0099a1c5e.png)

<h3 id="FZ52q">1.4.2、随机采样方法</h3>
但是概率最大的文本通常是最常见的文本。这些文本大多是“废话文学”——重复且平庸的文本。为了增加生成文本的多样性，随机采样的方法在预测试时增加了随机性。在每轮预测时，其先选出一组可能性高的候选词，然后再按照其概率分布进行随机采样。

**Top-K采样方法**

将概率最高的词进行归一化

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740125324705-398c436e-4f57-4ede-9ddc-9c78af02171a.png)

但是当候选词的分布的方差较大时，可能会导致预测选到概率较小、不符合常理的词。当候选词的分布的方差较小时，固定尺寸的候选集中无法容纳更多的具有相近概率的词，导致候选集不够丰富。

**Top-p采样方法**

添加一个阈值，对于高于这个阈值的词加入进来，进行归一化再选择。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740125694398-acb49cd8-e516-4d1b-8cb1-55d61837594c.png)

Top-p采样可以避免选到概率较小、不符合常理的词。其还可以容纳更多具有相近概率的词，增加文本的丰富度。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740125801117-ddb105cb-5b83-4a82-a674-6cfb4cba576a.png)

<h3 id="xOdpu">1.4.3、Temperature机制</h3>
Top-K采样与Top-P采样的随机性是由语言模型输出的概率决定的，不能自由调整。但是在不同场景中，对随机性的要求可能不同。，引入Temperature机制可以对解码随机性进行调节。（在归一化的数据中乘以一个系数，通过调节系数的大小来调节输出的随机性。即调节概率分布的平滑程度。）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126032909-dec8c25f-81f9-4998-aef4-c208895d3007.png)

<h2 id="AbPqm">1.5、语言模型的评测</h2>
语言模型的评测大概分为两类：内部评测和外部评测。

内部评测没有具体的评估指标（自我感觉），外部评测是指依赖具体的任务，含有具体的指标。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126423565-481d8dc3-c951-4ee1-b7fb-acf09c7b1726.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126522928-a482ba8d-30a3-4368-8093-37c924ab9f3f.png)

统计指标的评测可以看作是统计词元的频率进行评测

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126685759-a09fb2ba-1da5-4cd6-b377-8bd610e43b59.png)



![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126802464-da17d361-ba19-46f4-8261-fdcba1da58cd.png)

基于上下文嵌入的评测是通过计算词元的Embeding的相似度来进行评测

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126932660-4bb08e44-522f-4199-a4f6-b11cdfe6d2aa.png)

使用大模型来评测大模型

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740126996376-39d39d87-ce96-4573-9434-910d20c40580.png)

<h1 id="mpBTJ">二、大语言模型架构</h1>
上下文学习：指大模型在某些任务中无需额外的训练，仅通过上下文信息的实例或者提示即可理解任务并生成输出。

常识推理能力：指大模型基于常识知识和逻辑进行理解和判断的能力

逻辑推理能力：指大模型基于给定的信息和规则进行合乎逻辑的判断和结论的能力

<h2 id="PvFU8">2.1、大数据+大模型 -> 新智能</h2>
基于大模型性能的提升，，有着一系列关于模型能力与参数/数据规模之间的定量关系作为理论支撑，即扩展法则。其中以OpenAI提出的Kaplan-McCandlish(KM)法则以及DeepMind提出的Chinachilla法则最为著名。

<h3 id="vGvzM">2.1.1、大数据+大模型 -> 能力增强</h3>
**KM扩展法则**

OpenAI团队通过实验拟合出神经网络模型的性能L与数据规模D以及模型规模N之间的函数关系，提出了KM法则。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740128323640-ea6ce9a9-b3d7-4031-9b46-99d3706ceefa.png)

模型性能与模型规模、数据规模两个因素高度相关。为了提高模型性能，模型规模和数据规模应该同步增加。但相对而言，模型规模的优先级应该更高。

Google团队在KM法则的基础上提出了chinchilla法则。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740128649876-daea375e-9f2c-4306-ad2d-c278854b8782.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740128701306-bb5da302-1203-4823-b06a-760bcb883803.png)

<h3 id="WCUoD">2.1.2、大数据+大模型 -> 能力拓展</h3>
Transformer灵活的并行架构为训练数据和参数的拓展提供了模型基础，推动了大模型的发展

在Transformer模型的基础上衍生出了三种主流的模型架构

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740129093985-54dd06fa-df59-4daf-9e3d-f09af4a285fb.png)

**Encoder-only架构**

只含有Transformer的编码器部分，分别是输入编码部分，特征编码部分以及任务处理部分。

Encoder-only架构，将输入序列编码为向量表示，适用于分类、标注等任务。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740129396810-9fbba208-d916-4850-9638-144998e3bbb5.png)

**Decoder-only架构**

只含有Transformer的解码器部分，分别是输入编码部分，特征解码部分以及输出生成部分。核心特点在于忽略了每个解码模块中的交叉注意力子模块。其中解码模块对下文有一个遮盖的效果（因为要解码出下文的内容，不能提前泄露下文的内容）

仅使用Decoder部分，直接生成目标序列，适用于生成任务。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740130218265-234a1a79-6d6e-46af-b4cf-0d11c46cb3d3.png)

<font style="color:#DF2A3F;">为什么仅Encoder架构的生成过程不属于解码？</font>

1. 输入编码
    - 输入序列通过Encoder转换为向量表示。例如，BERT将句子编码为一组向量。
2. 后处理
    - 线性变换：将Encoder输出的向量通过一个线性层映射到目标空间（如词汇表大小）。
    - 条件生成模型：根据Encoder输出的向量，使用生成模型（如MLM、条件GAN）生成目标序列。
3. 生成目标序列
    - 通过后处理直接生成目标序列。例如，在BERT的填空任务中，模型直接预测掩码位置的词。

<font style="color:#DF2A3F;">实例：</font>

1. 文本分类（非生成任务）

+ 输入：`"这部电影很棒"`
+ Encoder输出：向量 `[0.1, 0.7, -0.3, ...]`
+ 后处理：通过线性层将向量映射到分类标签（如“正面”）。
+ 输出：`"正面"`

2. 文本生成（生成任务）

+ 输入：`"Translate 'Hello' to French"`
+ Encoder输出：向量 `[0.2, -0.5, 0.8, ...]`
+ 后处理：通过条件生成模型生成目标序列。
+ 输出：`"Bonjour"`

<font style="color:#DF2A3F;">decoder的生成过程</font>

+ Decoder逐词生成目标序列，每一步都依赖Encoder的输出和已生成的部分序列。
+ 这个过程是显式的解码过程，通常使用自回归生成（如RNN、Transformer Decoder）。

**Encoder-Decoder架构**

该架构本身可以看作是一个Transformer,同时选取了Transformer架构中的编码器和解码器，并采用了其中的交叉注意力机制来实现二者的交互 。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740662762942-3ee6f7f1-9fb8-420b-a3e2-3c5ce810045e.png)

**三种架构的对比**

下图是三种架构的注意力矩阵的对比

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740662834838-d547d9c1-eef1-45fa-978e-6cb8219bc611.png)

 Encoder-only架构会关注所有的词之间的关系

 Decoder-only架构是根据上一个词预测下面的词，所以会遮盖掉后面的词（Mask- attention）

Encoder-Decoder架构：原始的问题的输入是全部注意到的，下三角是一个生成的部分。（Mask-attention）

<h2 id="Q2vZa">2.2、基于Encoder-only架构的大语言模型</h2>
当前，最常用的基于Enocder-only架构的模型是BERT及其变体，如RoBERTa、ALBERT等。

BERT开创性的提出掩码语言模型（Masked Language Model，MLM）和下文预测（Next Sentence Prediction，NSP）两种任务来学习生成上下文嵌入。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740663690160-c179b486-f057-45fb-bdf1-9d79b25bcc5d.png)

<font style="color:#000000;">RoBERTa移除了BERT中的下文预测任务，并将掩码语言建模任务强化为动态掩码语言建模，从而增加模型训练的多样性，帮助模型学习到更丰富的上下文信息。</font>

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740663972014-7f890776-fbee-41cb-8f85-2e8b0779f05a.png)

ALBERT模型结构通过参数因子分解和跨层参数共享显著降低了模型的参数量。

对于Encoder-only架构的模型而言，参数主要来源于词嵌入矩阵以及Encoder块。其中，前者的参数量占20%，后者的参数量占80%。

对于Embedding模块，ALBERT采用参数因子分解的方式减少参数量。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740664224116-212e85e6-0341-43ab-901b-e316fe7faf3e.png)

对于Attention与FNN模块，ALBERT采用跨层参数共享的方式减少其参数量。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740664418859-36328861-a31f-4c45-8651-cab0f9bd76e9.png)

Encoder-only架构的优势：得益于双向编码模型的全面注意力机制，基于Encoder-only架构的模型在需要深度理解的任务中展现出了卓越的能力。

其中包括全面的注意力机制：双向编码模型融合了从左往右的正向注意力以及从右往左的反向注意力，能充分捕捉每个Token的上下文信息。

<h2 id="DfzKI">2.3、基于Encoder-Decoder架构的大语言模型</h2>
Encoder-Decoder在原先Encoder-only架构的基础上添加一个Decoder组件，使其能够生成连贯的输出。从而缓解Encoder-only架构在生成式任务上的局限性。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740818335497-64babeca-0cf1-434d-8d66-2e59a907bc83.png)

Encoder-Decoder架构主要包含编码器和解码器两部分

编码器：由多个编码模块堆叠而成，用于将输入序列转变为固定长度的上下文向量，包含：自注意力模块、全连接前馈模型。

解码器：由多个解码模块堆叠而成，基于上下文向量自回归的生成输出序列，包含：自注意力模块、交叉注意力模块、全连接前馈模型。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740818605933-39881a8a-7a96-4dab-94b6-35d36a09b828.png)

完整的Encoder-Decoder架构一共包括三种注意力模块：

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740818637190-51ef3bef-b581-4c2d-b23d-c929a7b47807.png)

其中编码器的自注意力模块是双向的，用以更好的理解上下文。

解码器的自注意力模块是单向的，用以更好的生成下文（也就是说需要将下文掩盖掉）

交叉注意力模块就是将Encoder得到的上下文向量进行交叉编码到Decoder里面。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740818923020-d86ee10f-423d-4ced-a64b-a1fc6c2a67e9.png)

交叉注意力模块：因为解码器需要根据编码器的上下文信息解码出相关的信息。

Encoder-Decoder架构的代表模型是BART和T5。  
![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740970385613-2661d9b1-fb45-40f6-96b6-b76b1c445aa4.png)

<h2 id="y9Fy7">2.4、Decoder-only架构</h2>
常用于开放式生成任务中，输入序列通常较为简单或者不明确。

Decoder-only架构去除了Transformer中的编码器部分，具有极其简化的架构设计和强大的可拓展性，被广泛的应用于大语言模型。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740971127680-4ca4f94d-1360-422b-a2c5-d33468519d63.png)

Decoder-only架构中最常见的是GPT系列以及LLaMA系列。

LLaMA1模型架构采用了与GPT1同样的网络架构，但在原始词嵌入模块、注意力模块和全连接前馈模块上进行了优化。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1740971902003-f327713c-8a62-4141-bc4b-8d52c46cd392.png)

<h2 id="EaUql">2.5 Mamba原理</h2>
Transformer的输入窗口长度有限，且模型规模随着输入序列长度平方次增长。在处理长序列的时候，计算成本高。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741518095635-ef17ac82-56e4-4a6c-8ba3-7b74d55b1b83.png)

为了避免模型规模随着序列长度二次增长，同时克服RNN的局限性，研究者提出了多种RNN变体。其中基于选择状态空间模型(Selective State Space Model，SSSM)的Mamba模型引发广泛关注。

**本节为非Transformer架构，暂不整理。**

<h1 id="zwu9O">三、Prompt工程</h1>
随着语言模型在规模上的提升，从T5模型开始，应用大语言模型处理下游任务时，开始从“预训练-微调-预测”范式。转向灵活的“预训练-提示(Prompt)预测”范式

Prompt是指用于指导生成式人工智能模型执行特定任务的输入指令，这些指令通常以自然语言文本的形式出现。Prompt的核心目的是清晰地描述目标任务，并适当的提供解决该任务所需的辅助信息。

一个标准的Prompt能显著提升模型生成回答的质量，一个标准的Prompt通常包含任务说明、问题、上下文、输出格式等四个基本元素。

<h2 id="Pkooo">Prompt的分词向量化</h2>
在构建合适的 Prompt 之后，用户将其输入到大语言模型中，以期得到满意的生成结果。但是，语言模型无法直接理解文本。在 Prompt 进入大模型之前，需要将它拆分成一个 Token 的序列，其中 **Token 是承载语义的最小单元，标识具体某个词**，并且每个 Token 由 Token ID 唯一标识。将文本转化为 Token 的过程称之为分词（Tokenization）。如下图所示，经过分词处理后会变成一个Token序列，每一个Token有对应的TokenID。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741658149099-ff34a7fc-5933-4a57-87a5-4cdd01c57e73.png)

为实现有效的分词，首先需要建立一个包含大语言模型所识别的所有Token词表，并根据该词表进行句子拆分。在构建大语言模型的词表时，分词器依赖于分词算法，如BBPE（Byte-Level Byte Pair Encoding）、BPE和WordPiece等，这些算法通过分析语料库中的词频等信息划分Token。这里以BBPE算法为例阐述分词过程：

1. 初始化词表：首先，将所有字符按照其底层编码拆分为若干字节，并将这些单字节编码作为初始词表的 Token。
2. 统计词频：接下来，统计词表中所有 Token 对（即相邻 Token 的组合）的出现频率。在初始阶段，Token 对即为相邻字节的组合。
3. 合并高频 Token 对：然后，选择出现频率最高的 Token 对，将其合并成一个新的 Token 并加入词表。
4. 迭代合并：重复步骤 2 和步骤 3，不断迭代合并，直至达到预设的词表大小或达到指定的合并次数

在完成分词之后，这些 Token 随后会经过模型的嵌入矩阵（Embedding Matrix）处理，转化为固定大小的表征向量。这些向量序列被直接输入到模型中，供模型理解和处理。在模型生成阶段，模型会根据输入的向量序列计算出词表中每个词的概率分布。模型从这些概率分布中选择并输出对应的 Token，这些 Token 再被转换

为相应的文本内容。

上述通过分词技术将文本分割成 Token，再将 Token 转化为特征向量，在高维空间中表征这些文本的处理流程，使得语言模型能够捕捉文本的深层语义结构，并有效地处理和学习各种语言结构，从简单的词汇到复杂的句式和语境

<h2 id="SacUY">3.1、上下文学习</h2>
上下文学习是大语言模型一种新的学习范式，通过构造特定的prompt，使得语言模型理解并学习下游任务。相比较于传统的监督微调，其不需要更新模型参数，可以快速的适应下游任务。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741662779521-33f3b7bb-c682-4876-b171-3915c578d2b2.png)

大语言模型在预训练阶段从大量文本中学习潜在的概念。当运用到上下文学习进行推理时，借助任务说明或演示实例来”锚定“其在预训练阶段所习得的相关概念，从而进行上下文学习，并对问题进行预测。

根据实例数量的不同，上下文学习可以分为三类：零样本（Zero-shot）上下文学习、单样本（One-shot）上下文学习和少样本（Few-shot）上下文学习。

鉴于不同方法对实例选择依据的侧重有所不同，现有的实例选择策略大概有三类：直接检索、聚类检索、迭代检索：

**直接检索：**在筛选实例时，检索器依据特定的评分标准对实例进行排序，然后选取排名靠前的K个实例。代表性的方法是KATE。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741663325391-416f21c0-13e7-4ef6-8aba-791fb367fccd.png)

**聚类检索：**把所有实例划分为K个簇，让相似的实例聚集在一起，而后从每个簇中选取最为相似的示例，最终获取K个实例。代表性方法是Self-Prompting。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741663457369-36261860-875e-4c99-b318-10744d36279e.png)

**迭代检索：**检索过程是迭代的，下一个实例的选择依赖于当前的问题和已选择的实例。代表性方法是RetICL。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741663573257-f133c948-a553-4be0-b665-3751028b8a38.png)

<h2 id="S51IU">3.2、思维链</h2>
![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741761480149-66047b67-e450-459a-ad43-a0a8c49ad3d7.png)

思维链（Chain-of-Thought, CoT）通过在提示中嵌入一系列中间推理步骤，引导大语言模型模拟人类解决问题时的思考过程，以提升模型处理System2任务的能力。

<h3 id="zJLP5">3.2.1、思维链的分类</h3>
标准的CoT方法上，可以归纳为三种模式：按部就班、三思后行和集思广益

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741761711403-42218b5e-324d-4fb4-8850-942fb2b4afea.png)

<h4 id="omCBH">按部就班模式</h4>
按部就班模式强调的是逻辑的连贯性和步骤的顺序性。在这种模式下，模型像是在遵循一条预设的路径，每一步都紧密依赖于前一步的结论

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741761875324-1c9475f9-d0e3-49d2-b00f-9842b6d16c00.png)

**Zero-Shot-CoT**

** **Zero-Shot CoT经过简单的提示，如“让我们一步步思考”，引导模型自行生成一条推理链，在多个推理任务中展现出了与原始样本CoT相媲美甚至更优的性能。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741762126366-b24a1ae5-e6a7-4e45-8797-87facc9b398b.png)

**Auto-CoT**

在Zero-Shot CoT的基础之上，Auto-CoT引入了与待解决问题相关的问题及其推理链作为示例，以继续提升CoT的效果。Auto-CoT无需人工标注成本，但是性能超过了需要手工标注的CoT和无需手工标注的Zero-Shot-CoT/

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741762340955-2a648489-799a-42a4-98e4-5149ca09f8f9.png)

<h4 id="teaBa">三思后行模式</h4>
三思后行模式在决策过程中融入了审慎和灵活性。在这种模式下，模型在每一步会停下来评估当前的情况，判断是否需要调整方向。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741762535265-bd37631b-fda8-4971-8f52-ef7815b5019b.png)

**Tree of Thoughts**

为了模拟人类在做System-2任务时审时度势的过程，Tree-of-Thought(ToT)将推理过程构造为一棵思维树，在构造树的过程中，允许模型在遇到困难或者不确定时进行回溯和重新选择。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741762724766-542f59dc-0037-4b77-95de-b5593c5cbbf8.png)

ToT从拆解、衍生、评估、搜索四个角度构造思维树。

1. 拆解：将复杂问题拆分为多个简单的子问题，每个子问题的解答过程对应一个思维过程。
2. 衍生：模型需要根据当前子问题生成可能的下一步推理方向。衍生有两种模式：样本启发和命令提示。
3. 评估：利用模型评估推理节点合理性。根据任务是否便于量化评分，选择投票或者打分模型。
4. 搜索：从一个或者多个当前状态出发，搜索通往问题解决方案的路径。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741764483312-b930f660-4c86-41c5-bc5b-ea81dd9540c0.png)

<h4 id="I2UH7">集思广益模式</h4>
集思广益模式强调的是通过汇集多种不同观点和方法来优化决策过程。在这种模式下，模型不仅仅依赖于单一的推理路径，而是通过探索多种可能的解决方案，从中选择最优的答案。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741764623447-fb882f95-f143-49af-8e64-bbdd110bd8bd.png)

**Self-Consistency**

Self-Consistency引入了多样性的推理路径并从中选择最一致的答案，从而提高了模型的推理准确性。Self-Consistency不依赖于特定的CoT形式，可以与其他CoT方法兼容，共同作用于模型的推理过程。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741764837747-2e42bdf3-d165-48bc-897b-99c860dbab22.png)

Self-Consistency的过程可以分为三个步骤：推理路径生成、汇总答案和选择答案。

1. 推理路径生成：在随机采样策略下，使用CoT或者Zero-Shot CoT的方式来引导大语言模型针对待解决问题生成一组多样化的推理路径；
2. 汇总答案：针对大语言模型生成的每个推理内容，收集其最终的答案，并统计每个答案在所有推理路径中出现的频率；
3. 选择答案：选择出现频率最高的答案作为最终的、最一致的答案。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741765069020-b1917427-edfd-452f-8e54-ef0befb08ebc.png)

**Universal Self-Consistency**

 Self-Consistency依赖于答案提取过程来聚合多个解决方案，不适用于自由形式的答案，例如文本摘要等任务。Universal Self-Consistency利用大模型本身来选择最一致答案，显著拓宽了Self-Consistency的使用场景。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741765253552-36c3f184-9aca-4a9a-8ac5-14e5a258c8d9.png)

<h3 id="AR3du">补充</h3>
<h4 id="cRlth">训练时的大规模强化学习</h4>
谷歌提出的SCoRe设计两阶段学习方案，训练语言模型学会自我反思与纠正的方法，通过增大训练时计算，提升模型的思维片段自我纠正能力以及长推理能力。

阶段一：针对模型第一次差强人意的回答，进行微调，时期第二次能回答的更好。

阶段二：设计面向自我纠正的多轮强化学习策略优化模型，进一步增强模型能力。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741765864518-18671327-0f75-401c-88e4-82a2355cb99a.png)

<h4 id="sJlDI">训练时的大规模搜索采样</h4>
谷歌另一个工作又提出了模型测试时，利用奖励模型指导大规模搜索，探索多种思维片段，从而增大测试时计算，显著增强模型复杂推理问题的能力。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741766020785-90797a77-bc92-41dc-aad3-96a9f1b793ae.png)

<h2 id="QrYP0">3.3、Prompt技巧</h2>
**一个标准规范的Prompt通常由任务说明，上下文，问题，输出格式这几部分中的一个或者几个组成。**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741826209183-d1ea3e68-3dba-477e-ac19-91064c35246a.png)

**一个清晰、具体的任务说明能够确保模型准确理解任务要求，产生符合预期的输出。**

+ 明确的动词：选择能够清晰表达动作的动词，如”判断“、”分类“，避免模糊的动词如”处理“、”操作“。
+ 具体的名词：使用具体的名词来定义任务的输出或者目标，例如”积极/消极“，”YES/NO“。
+ 简洁明了：任务说明应该简洁且直接，避免冗长或者复杂的句子，是模型能够快速抓住任务的核心要求。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741826439095-8bd99a87-237a-42a0-921d-37f7cc768721.png)

**一个丰富且清晰的上下文能够显著提升模型回答的准确率。**

+ 上下文丰富体现在其内容可以是与问题直接相关的背景信息、具体的演示实例，或是对话历史等。

 ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741826623891-9263e193-96ca-4cf8-85c0-d97fc03afb0e.png)

+ 上下文的清晰体现在上下文信息必须与问题紧密相关，避免包含冗余或不必要的信息。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741826755277-2024477c-0621-4686-adf4-f8841e002e5d.png)

**规范的输出格式对于确保模型输出的可用性至关重要**

+ 如果期望的输出格式十分具体，可以提供输出格式的具体示例，使模型按照期望的输出格式输出内容。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741826945634-e8b6acda-375b-4b19-86ff-24acfb8af595.png)

**清晰的排版有助于避免歧义，加深模型对Prompt的理解，准确捕捉任务的关键信息，从而提高其执行任务的准确性。**

+ 使用一致的分隔符：选择并坚持使用一种或几种分隔符（如“#”、“###”、“—”等），以区分不同的 Prompt 部分。
+ 合理使用空白和缩进：通过增加空白行和适当的缩进，增强 Prompt 的可读性，帮助模型区分不同的内容块。
+ 清晰的标题和子标题：为每个部分提供清晰的标题或子标题，使模型能够快速识别每个部分的主题。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741827180000-df9bbb23-5bc6-4f0e-9d94-5b5daf9e4fc5.png)

**合理归纳提问、适时使用CoT、善用心理暗示**

<h2 id="g5E8P">3.4、Prompt工程应用</h2>
<h3 id="QvsGh">3.4.1、自然语言接口——Text-to-SQL/代码生成</h3>
Text-to-SQL旨在将数据库项下的自然语言问题转化为相应的结构化查询语句(SQL)，该语言可以对关系数据库执行，从而为关系数据库实现低代码操作。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741871498151-f6f71307-a9c8-41b3-be99-7ee7822de8ee.png)

<h3 id="iTVL6">3.4.2、大模型支撑的数据合成</h3>
大模型的性能高度依赖于训练数据的质量，而获取高质量的数据资源变得越来越困难。通过Prompt工程技术，利用大语言模型强大的思维能力、指令跟随能力，来合成高质量数据的方法已成为当前研究的热点议题。

**数据合成——Self-Instruct**

Self-Instruct通过Prompt工程技术构建Prompt，多步骤的调用大语言模型，依据少量指令数据，合成大量丰富且多样化的指令数据与对应的指令回答。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741871880880-b18261dc-ae02-4e35-bfb0-eca1d1c712aa.png)

**数据合成——Evlo-Instruct**

Evlo-Instruct通过Prompt工程技术构建Prompt，引导大模型将初始指令逐步重写为更为复杂的指令，最终获得复杂且多样的指令数据。再根据指令数据生成对应的指令回答，构建可用于微调大模型的指令数据集。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741872048866-4deb272c-bac0-4f13-92ef-c79b9c2227e6.png)

<h3 id="ZSlfd">3.4.3、大模型增强的搜索引擎</h3>
大语言模型增强的搜索引擎利用Prompt工程技术，引导大模型理解用户的查询意图，提供更加个性化和精确的搜索结果。

<h3 id="Fwtep">3.4.4、大模型赋能的智能体</h3>
智能体（Agent）是一种能够自主感知环境并采取行动以实现特定目标的实体。在大模型时代，智能体迅速发展，通过Prompt工程驱动大模型作为智能体的大脑，让创建广泛应用且实用的智能体成为可能。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741911930740-366a081b-7520-4931-816c-3e849fe4fd42.png)

经典的智能体通常由大语言模型基座和四大模块组成，分别是配置模块（Profile）、记忆模块（Memory）、计划模块（Planning）、和行动模块（Action）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1741912081207-80222e39-a707-4d44-8aa4-5f86d143a140.png)

智能体分为单智能体和多智能体

单智能体：仅包含一个智能体，交互相对简单，适用于较简单的任务。Prompt设计主要用于引导模型执行特定的任务，且主要围绕单个上下文。

多智能体：包含多个智能体，每个智能体都有设定的角色和任务，交互复杂度高。他们协调、合作甚至竞争，以实现共同或者各自的目标。Prompt除了设计用于引导模型执行任务，还用于智能体之间的信息传递，且存在多个上下文。

<h3 id="eUWoG">3.4.5、大模型驱动的具身智能</h3>
大语言模型是AGI的重要基石和智慧引擎，机器人是大语言模型走向世界的物理载体。通过机器人的实体感知和行动能力，将大语言模型的强大语言理解和生成能力转化为实际的物理交互，实现从虚拟智能到具身智能的跨越。

<h1 id="q61hr">四、参数高效微调</h1>
指令微调（Instruction Tuning）旨在对模型进行任务指令的学习，使其能更好的理解和执行各种自然语言处理任务的指令。指令微调需首先构建指令数据集，然后在该数据集上进行监督微调。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742173440470-6212cb43-6e1d-4f6d-bc5e-8765e6353ac0.png)

指令数据通常包括指令（任务描述）、实例（可选）、问题与回答。通常构造指令数据集有两种方式：（1）数据集成；（2）大模型生成；

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742173618616-09522438-eb91-4395-b0ad-2d2212cb2008.png)

**监督微调（SFT）**

基于构造的指令数据集，对大模型进行监督微调（Supervised Fine-Tuning）。对于现有的大语言模型，通常以自回归的方式进行训练。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742173727266-eb4a3d31-a4be-4d57-b7a8-3d8a0825c4bb.png)

然而全量监督微调需要更新所有模型参数，当面对用于庞大参数量的大模型时，全量监督微调会消耗大量存储和计算资源。

**参数高效微调**

为了解决全量微调的问题，参数高效微调（Parameter-Efficient Fine-Tuning,PEFT）避免更新全部参数，在保证微调性能的同时，减少更新的参数量和计算开销。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742174286335-a243f2cd-d600-4f8b-bd82-59b9b2869b9f.png)

主流PEFT方法可以分为三类：参数附加方法、参数选择方法、低秩适配方法

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742174684530-13b16b1c-6dd1-4f2d-82e8-f180acd22108.png)

<h2 id="T3kQ2">4.1、参数附加方法</h2>
参数附加方法通过增加并训练新的附加参数或模块对大模型进行微调。参数附加方法按照附加位置可以分为三类：加在输入、加在模型以及加在输出。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742174851299-d933b07f-7593-4142-b2ac-61e10f485cf4.png)

<h3 id="ddL3Z">4.1.1、加在输入</h3>
加在输入的方法将额外参数附加在模型的输入嵌入（Embedding）中，其中最经典的就是Prompt-tuning。这些额外参数无向量化词表。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175047853-6bfba6ab-4c4f-42cd-ba66-0a861b11e3d6.png)

这些额外的参数也称之为软提示（Soft Prompt）。不同于手工编写的硬提示（Hard Prompt），软提示会在训练过程中被动态的调整，其本质是可训练的，连续的嵌入。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175201507-ba082765-8f44-49bd-8e5a-627dc1cf68ac.png)

Prompt tuning

Prompt tuning引入软提示作为模型输入的一部分，与实际的文本数据一起被送到大模型中。在微调过程中，仅**软提示的参数会被更新。**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175370521-1347fb1b-4d79-4c47-b23d-c6861deda50c.png)

设置合适的软提示长度和合理初始化软提示对Prompt-tuning至关重要。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175442205-49304a09-e3e3-496b-a58d-fd810719af70.png)

加在输入的优势：将额外参数附加到模型输入有以下优势：内存效率高、多任务能力、缩放特性。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175696894-4d50f575-0f86-44b2-9c67-a4bc8146533c.png)

<h3 id="NmCq2">4.1.2、加在模型</h3>
加在模型的方法将额外的参数或模型添加到预训练模型的隐藏层中，其中经典的方法有Prefix-tuning、Adapter-tuning等。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175806888-42b8e96d-c8be-43c3-9823-c75e7428fa95.png)

**Prefix-tuning**

Prefix-tuning与Prompt-tuning十分类似，但是对软提示的处理有所不同。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742175857777-9d915128-f752-40b6-b8f2-f2aa318f82a8.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176055656-b88adc60-4139-4d7b-a40d-9f703814bf02.png)

**Adapter-tuning**

Adapter-tuning通过在预训练语言模型的每个多头注意力层和全链接层后**插入新的可学习神经网络模块（称为适配器，Adapter）**来实现扩展。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176267844-05f5bb6c-6c98-4016-9e70-30926e9aa959.png)

适配器模块通常采用瓶颈（Bottomneck）结构，即**一个上投影层、一个非线性映射和一个下投影层组成的全连接模块。**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176417237-9c8cea0e-fee5-4762-bd2d-f3867a1c8474.png)

加在模型的方法有以下优势：参数效率高、任务适应性强、保持预训练知识。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176546600-9509a9e1-3ae4-41c6-a1ba-fee379c311f3.png)

<h3 id="XzBXN">4.1.3、加在输出</h3>
**代理微调**

代理微调（Proxy-tuning）提供了一种轻量级的解码时（Decoding-tuning）算法，让我们只需要微调较小的专家模型，且只需要访问大规模语言模型的输出词汇表预测分布，来实现对大规模语言模型的定制化调整。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176804630-9d107944-aa35-4d09-bfba-a52946125607.png)

给定代微调的代理模型（需要适配到下游任务的模型）M以及较小的反专家模型（Anti-export model）M-，这两个模型需要相同的词汇表。我们对M-进行微调，得到微调后的专家模型（Export model）M+。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742176938612-ac7a9cac-ab25-4e8e-90fa-d06f507c149f.png)

在每一个自回归生成的时间步中，代理微调首先计算专家模型M+和反专家模型M-之间的logits分布差异，然后将其加到代理模型M下一个词预测的logits分布中。具体如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742177202197-6ed48029-efa7-4176-a7a5-5300ac70ac1c.png)

加在输出的参数高效微调方法可以在一定程度上解决大规模语言模型微调和黑盒模型微调两个问题。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742177478141-cdf90ed6-5fe0-41fc-b072-887cb0c733c0.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742177777408-1e2144a3-1c62-4b30-a9e8-543d3220f084.png)

<h2 id="I5zBz">4.2、参数选择方法</h2>
参数选择的方法选择模型中的部分参数进行微调。参数选择方法分为两类：基于规则的方法和基于学习的方法。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742177892170-5f1ccd58-ca90-4b95-b285-e5fea7227c6e.png)

<h3 id="EY5oe">4.2.1、基于规则的方法</h3>
BitFit方法

基于规则的方法中最具代表性的方法是BitFit，BitFit通过仅优化神经网络中的每一个层的偏置项（Biases）以及任务特定的分类头来实现参数高效微调。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742178042063-1ea80314-9a1d-48dd-8075-f175288f4ffa.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742178098079-acd2e8c3-c88b-472e-98e6-1f000b84ad83.png)

<h3 id="QicYz">4.2.2、基于学习的方法</h3>
基于学习的方法在模型训练过程中自动地选择可训练的参数子集。其中最为经典的方法是Child-tuning。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742178237146-75d7900c-b7af-470b-861c-32928b563f73.png)

该方法是通过梯度掩码矩阵（由0/1构成）策略实现仅对选中的子网络进行梯度更新，而屏蔽子网络梯度以外的梯度，从而实现对参数微调的选择，达到参数高效微调的目的。

在梯度下降过程中，对掩码矩阵进行学习。学完之后只更新掩码矩阵中是1的那些梯度参数。

<h2 id="WGnkc">4.3、低秩适配方法</h2>
<h3 id="n6HOO">4.3.1、低秩适配方法</h3>
低秩适配方法（Low-rank Adaptation Methods）通过低秩矩阵近似原始权重更新矩阵，并仅微调低秩矩阵，以大幅度降低模型参数量。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742179111076-1f13151d-fc34-4f20-bb5e-f85b838fa965.png)

本征维度假设

本征维度假设：在预训练语言模型微调时，仅在一个低维子空间中进行参数更新，也能实现与完整参数更新类似的性能。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742179253511-2affd5d9-e959-4f26-ad58-ca9ece092800.png)

<h4 id="KnuDo">4.3.1.1、LoRA</h4>
低秩适配（Low-rank Adaptation,LoRA）将d×k参数更新矩阵低秩分解为一个r×k的矩阵和一个d×r的矩阵（r<<min(d,k)），冻结原模型参数，仅微调两个小矩阵。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742179502742-284f668a-30fe-4bd0-8bee-ff7466493102.png)

LoRA的性能受多种因素的影响，主要包括权重初始化、秩(r)、以及施加位置。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742179655764-c5ed3f8d-6240-449d-bd34-482536441acd.png)

**LoRA-权重初始化**

LoRA权重初始化对训练有较大影响。通常将投影矩阵B用0初始化，投影矩阵A用高斯分布初始化。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742179796709-74f51da9-0ae6-4922-b740-619d796e1fcd.png)**LoRA-秩的影响**

在简单任务中，LoRA使用较低的秩就能保证不错的性能；在跨领域迁移或复杂任务时，较高的秩往往表现更好。

**LoRA-施加位置的影响**

在同样的参数预算下，LoRA施加在不同位置的权重矩阵上会带来不同的影响。

**微调时内存占用**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742470249507-90c2d14e-661f-4e30-83e7-0e8d54efed92.png)

LoRA在梯度内存和优化器内存上进行了优化，微调时有较高的参数效率。

LoRA主要有三个方面的优势：参数效率、插件化效率、跨任务泛化。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742470620607-2902cdf8-c506-4967-823f-3d892d01a2fe.png)

<h4 id="C1CmR">4.3.1.2、AdaLoRA（性能优化）</h4>
在微调Transformers时，不同的模块和层中的权重矩阵的重要性不同。但LoRA通常会均匀的分配增量更新的预算，忽略了这种差异性。

AdaLoRA将参数更新矩阵参数化为奇异值分解（SVD）的形式，再通过奇异值剪枝动态的调整不同模块中LoRA的秩，秩越大代表该模块参数越重要。

<h4 id="Prci8">4.3.1.3、LoRAHub（泛化性优化）</h4>
可以通过组合在不同任务上训练的LoRA模块来提高模型对未见任务的适应性。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742471187778-6cc0e646-bfec-4ae6-ab44-16a43af3a723.png)

LoRAHub提供了一个可用的多LoRA组合的方法框架。该方法将已有的任务上学习到的LoRA插件进行组合，从而获得解决新任务的能力。

<h4 id="kCQjC">![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742471334359-3a3d9d6d-fe99-4e81-a3bb-0ef16745342a.png)4.3.1.4、QLoRA（训练改进）</h4>
模型量化是将模型参数从高精度表示（16位浮点数）转换为低精度表示（4位整数）。大模型量化结合LoRA技术可以显著减少存储和计算成本，同时保持模型的性能，提高部署效率。![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742471573482-14921508-fbcb-4884-be2c-7bec9a51cec5.png)

QLoRA将原始模型权重从16-bit量化为4-bit存储，只将LoRA参数保持在16-bit，从而节省了权重内存。在前向传播和反向传播中，为了保持性能，动态的将模型参数反量化到16-bit进行计算，计算后释放。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742471770884-cf325305-89c6-451a-9cd5-a8e611ca9dc9.png)

4.3.1.5、S-LoRA（推理改进）

在推理时，LoRA参数可以直接合并到预训练的模型参数中，从而避免额外的开销。但是，这样的参数合并的方法，导致在多个LoRA插件共同发挥作用的场景中，一次只能用一个插件，导致GPU利用率低等问题。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742472105317-d7ede5d2-8289-4f7a-a59d-8e902e50b410.png)

S-LoRA将输入与LLM和LoRA参数的运算分离，两者分别计算完成之后再求和得到最终结果。使得不同LoRA的请求可以放在一个batch中，从而增加了请求执行的并行度，提升了GPU的效率。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742472237234-629d5827-fce0-4857-a586-774dc46c0a8b.png)

<h2 id="VngbB">4.4、PEFT在预训练中的应用</h2>
在预训练阶段，PEFT方法可以在冻结大部分模型参数从而节省内存和计算的情况下，保持与全量训练相近的性能。

<h3 id="iVvke">4.4.1 ReLoRA</h3>
ReLoRA在LoRA的基础上通过多次重启（每隔一段训练步数将低秩矩阵与模型权重合并，并重新初始化生成的新的低秩矩阵）来得到比较好的训练效果。

<h1 id="IUk0h">五、模型编辑</h1>
大语言模型有时会产生一些不符合人们期望的结果，如偏见、毒性和知识错误等。偏见是指模型生成的内容中包含刻板印象和社会偏见等不公正的观点，毒性是指模型生成的内容中包含有害成分，而知识错误则是指模型提供的信息与事实不符。

可能解决的方案是重新预训练但成本过高或者是微调但有过拟合或者是灾难性遗忘等问题。

模型编辑技术针对特定的知识点对模型进行编辑，其旨在修正大语言模型使其输出期望结果，同时不影响其他无关的输出。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742473528985-9f712039-201e-42d7-8e77-5cf7d29eb55e.png)

<h2 id="vxIsQ">5.1、模型编辑方法分类</h2>
![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742474135335-e0922956-d768-480c-957f-181d0f1c2bef.png)

<h3 id="xDoSo">5.1.1、外部拓展法</h3>
外部拓展法的核心是将新知识存储到附加的外部参数或者外部知识库中，将其和原始模型一起作为编辑后的模型

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742474253155-f78ff2d2-08ba-4a91-83f4-bd3c3858f9c5.png)

<h4 id="OuDBy">5.1.1.1、知识缓存法</h4>
知识缓存法中包括三个主要组件，分别为编辑缓存、门控单元和推理模块。编辑缓存从当一个知识库，用于保存需要修改的知识。首先门控单元检查输入问题是否与缓存编辑中的知识点匹配，若匹配，将知识点与输入一同送入推理模型，得出答案；若不匹配，使用原始模型进行推理，得出答案。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742474505011-4c0090bb-db21-4d0a-a35c-30bfa0196a03.png)

<h4 id="M0kZm">5.1.1.2、附加参数法</h4>
附加参数法将外部参数插入到模型的特定位置中，冻结原始模型，只训练新引入的参数以修正模型。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742474819521-35cfc0c2-c591-41e4-97f5-3d4f46c015c6.png)

<h3 id="KF5ys">5.1.2、内部修改法</h3>
内部修改法的核心思想是通过更新原始模型的内部参数来为模型注入新的知识，在不增加物理存储负担的情况下直接优化自身，提高其在特定任务上的表现。

<h4 id="GEedS">5.1.2.1、元学习</h4>
是指模型“学习如何学习”的过程。元学习通过多个任务上的学习积累经验，也被称为元知识，从而指导模型更加高效的学习新任务。（学习一个网络如何指导学习）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742475094049-ee4806aa-0228-487e-a593-0affef47a8a5.png)

<h4 id="JG1eh">5.1.2.2、定位编辑法</h4>
定位编辑法对原始模型的局部参数进行编辑，先定位到需要修改参数的位置，然后修改关键参数。其定位过程基于对大模型中知识的存储机制的理解。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742475348892-70537570-1f59-466d-88cc-b1b8b6a36d36.png)

<h2 id="ilSLC">5.2、附加参数法：T-Patcher</h2>
T-Patcher是附加参数法中的代表性方法，能够不改变原始模型整体架构的情况下，通过在模型的最后一个Transformer层的全连接前馈层中添加额外参数（称为补丁），并对补丁进行训练来完成特定知识的编辑。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742475696128-92960c0b-9aa3-4178-af0e-0133070eddca.png)

在模型的不同位置添加补丁会影响编辑效果。有研究发现全连接前馈层可被视为键值存储体，是存储知识的关键模块。T-Patcher通过在最后一个Transformer层的全连接前馈层中添加补丁并精确控制其激活，能针对特定输入进行修正。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742475857438-7b9dd24f-49fc-4154-bed5-a7b88f7cbcd6.png)

T=Patcher在全连接前馈层中加入额外的键值对向量作为补丁，每个补丁的参数包中包括一个键向量kp，一个值向量vp和一个偏置向量bp.

<h2 id="SHsNr">5.3、定位编辑法</h2>
经过了一个因果跟踪实验（可以不必了解）得到了一个知识存储假设。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742476613204-1089086e-d959-4d8b-97f2-4182f853bb92.png)

知识存储假设：

ROME方法推测知识以键值映射的形式等价地存储在任何一个中间层的全连接前馈层中，并对语言模型中的知识存储机制做出了假设。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742476864187-5697dbd9-2a86-4fbd-863e-f13d3a5a35aa.png)

+ 首先，起始的 Transformer 层中的注意力层收集主体 s 的信息，将其汇入至主体的最后一个 Token 的向量表示中。
+ 接着，位于中间层的全连接前馈层对这个编码主体的向量表示进行查询，将查询到的相关信息融入残差流（Residual Stream）中。
+ 最后，末尾的注意力层捕获并整理隐藏状态中的信息，以生成最终的输出。

基于上述知识存储假设，ROME通过因果跟踪实验先定位出一个因果效应较强的全连接前馈层，然后对其进行编辑。编辑过程包含三个步骤：1.确定键向量；2.优化值向量；3.插入知识

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742477228716-4f2d1e66-0135-4199-8ddc-23d1ab90a622.png)

与 T-Patcher 相似，ROME 同样将全连接前馈层视为一个键值存储体。但不同的是，T-patcher 将上投影矩阵的参数向量看作键向量，将下投影矩阵的参数向量看作值向量，而 ROME 则是将下投影矩阵的输入向量看作键向量，将其输出向量看作值向量。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742477337059-2c5f19a7-00e6-41f0-af99-273bc3f34605.png)

**确定键向量k***，他应该编码着主体s。根据知识存储假设，ROME直接读取Last Subject Token在全连接前馈层激活函数之后的向量表示作为键向量。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742477479726-6fdd3452-fa53-482b-8cea-36fef2f62709.png)

**确定值向量v***,将待编辑的知识的（r,o）编码为s的属性。值向量是 Wproj 与 k∗ 运算后的期望结果，即编辑后全链接前馈层处理Last Subject Token的输出。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742477675627-80f94cac-384c-410e-9406-6bd4eb999fc0.png)

**插入新知识**，ROME将新知识的插入过程建模为一个最小二乘问题，并通过求解这个问题来确定Wproj的最优变化量。并且ROME将Wproj视为一个线性的键值存储体，其目标是向Wproj添加新的键值对（k*,v*），同时不破坏原有的映射关系。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742477970328-a03b22bf-afd8-4663-8e69-675f76683469.png)

<h1 id="BqCW4">六、检索增强生成(RAG)</h1>
<h2 id="D4HGu">幻觉现象</h2>
“幻觉现象”可能直接来自于训练数据的知识错误，也可能源于训练出的模型本身对知识的把握不足。

+ **幻觉现象-训练数据**
    - **知识过时：**由于训练数据中的时间滞后，其中的知识可能在模型训练后又发生了更新，导致模型内部知识过时。
    - **知识边界：**由于训练数据的有效性，无法覆盖所有范围，且知识在训练数据采集完成后仍会不断新增。
    - **知识偏差：**训练数据中可能包含不实与偏见信息。
    -  **对齐不当：**在模型与人类偏好对其阶段中，偏好数据标注不当可能引入了不良偏好。
+ **幻觉现象-模型本身**
    - **知识长尾：**训练数据中部分信息出现的频率较低，导致模型对这些知识的学习程度较低。
    -  **曝光偏差：**由于模型训练与推理任务存在差异，导致模型在实际推理时存在偏差。
    - ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742819609767-008e7a07-dd69-4a26-b96c-a79e8dc4297f.png)
    - **解码偏差：**模型解码策略中的随机因素（采样也是使用随机采样的方式进行的）可能影响输出的准确性。
    - ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742819712823-09d50d7c-e85a-4efe-976b-e5701b7de0f2.png)

面对上述出现幻觉现象的原因，通过使用检索与问题相关的信息进行辅助，从而有效缓解幻觉现象，大幅度提升模型的生成质量。（检索增强生成（RAG）的核心思想）

RAG（Retrieval-Augmented Generation），即检索增强生成，是一种从外部数据库中检索相关信息来辅助改善大模型生成质量的系统。一个基本的RAG框架主要包含知识检索和生成增强两大模块。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820090976-a095411f-90e2-416d-9a70-10185f1810cf.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820123269-f3288e78-9d69-46ef-a5bd-0c8dd3a6b483.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820156447-72ce2e49-ae7b-48cb-bfd6-3ece0d156978.png)

<h2 id="n3q0u">6.1、RAG架构分类</h2>
在RAG中，大模型根据参数进行感知和调节可分为“黑盒”模型与“白盒”模型。其中，闭源模型视为“黑盒”模型，而开源模型根据是否对参数微调既可以视作“白盒”模型也可以视作“黑盒”模型。

从是否对大模型参数进行更新的角度出发，RAG架构可以分为两大类：黑盒增强架构和白盒增强架构。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820500199-68329e5c-55ee-4c3a-a6f8-be3476661cd0.png)

**黑盒增强架构**

黑盒增强架构可根据是否对检索器进行微调分为两类：无微调和检索器增强

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820638755-0d12a7ef-6b49-4e09-a9ba-ffa8125ee354.png)

**白盒增强架构**

白盒增强架构也可以根据是否对检索器进行微调分为两类：仅微调大模型和检索器与大模型协同微调。（简称协同微调）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820753330-8794625e-2d75-4e96-8455-06843788940c.png)

<h3 id="T9NI2">6.1.1、黑盒增强架构详解</h3>
<h4 id="foFl6">6.1.1.1、无微调</h4>
检索器和大模型在RAG过程中参数不更新，二者直接组合使用来完成生成任务。代表性的方法是In-Context RALM。其直接将检索器检索到输入问题前作为上下文。

优点：检索器与大模型解耦，计算成本低。

缺点：大模型与检索器缺乏交互，完全依赖于大模型的指令跟随能力。整体效果难以保证

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742820968526-847e182b-1b8b-47bd-b1cb-50182412cb2f.png)

<h4 id="SmDnl">6.1.1.2、检索器微调</h4>
大模型参数固定，检索器参数根据大模型的输出进行更新，使检索器能够更好地适应大模型的需求。代表性的方法为REPLUG。其模型困惑度作为监督信号来训练检索器，以检索出能显著降低模型困惑度的文档。 

优点：更新检索器以适应大模型，无需更新LLM。

缺点：大模型参数固定，可能无法与检索器良好适配。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742821213893-030868d2-a1e6-4e91-961f-a99fcfa9ca1c.png)

<h3 id="n5KSm">6.1.2、白盒增强架构详解</h3>
<h4 id="tcz8V">6.1.2.1、仅微调大模型</h4>
检索器作为预先训练好的组件使其参数保持不变，大模型根据检索器输出对自身参数进行更新。通过交叉编码与大模型进行联动，将检索信息动态融合到大模型的隐藏状态中。

优点：能优化模型的语言生成能力，更好地检索外部信息。

缺点：微调资源需求高；微调效果依赖于原生的检索器性能。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742821462338-f41fb757-d882-460f-9fb5-d0ce511e715f.png)

<h4 id="vHkym">6.1.2.2、协同微调</h4>
在检索器与语言模型协同微调架构中，检索器与大模型的参数更新同步进行。代表性的方法为ATLAS。其使用KL散度损失函数联合训练检索器与大模型，以确保检索器输出的文档相关性分布与文本对模型的贡献分布一致。

优点：检索器与大模型能够在训练过程中深度交互，优化RAG性能。

缺点：资源需求高；实现与训练过程复杂。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742821770035-d2fce098-6ab4-441a-adc8-34bae32c0ab7.png)

 ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742821945732-fdde310f-f402-461e-a80b-f0dc8aab27f2.png)

<h2 id="oV4oz">6.2、知识检索</h2>
知识检索通常包括知识库构建、查询构建、文本检索和检索结果重排。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742822197292-e4b92f13-f226-4d3d-8cbd-0fab145cc83f.png)

<h3 id="aK8PX">6.2.1、知识库构建</h3>
**数据采集与预处理：**数据采集与预处理为构建知识库提供“原材料”。

**数据采集：**从不同渠道整合、转化多元数据资源，将其转化为统一的文档对象。例如维基百科语料库，采集内容不仅包括正文，还包括一系列元信息，如标题、目录、分类等。

**数据预处理**

+ **数据清洗：**清楚文本中的干扰因素，如特殊字符等；
+ **文本分块：**将大块文本分割成较小的单元，例如把一篇长文本分为多个短段落。
    - **固定文本分块：**按照预先固定的长度将文本进行分块。
    - **基于内容的分块：**根据待分块的内容，对文本进行更加贴合内容逻辑的分块。

**知识库增强**

知识库增强是通过改进和丰富知识库的内容和结构，为查询提供“抓手”，包括查询生成与标题生成两种方法。

+ **查询生成：**指的是利用大语言模型生成与文档内容紧密相关的伪查询。这些伪查询从查询的角度来表达文档的语义，可以作为相关文档的“键”，供检索时与用户查询进行匹配。通过这种方式，可以增强文档与用户查询的匹配度。
+ **标题生成：**指的是利用大语言模型为没有标题的文档生成合适的标题。这些生成的标题提供了文档的关键词和上下文信息，能来用来帮助快速理解文档内容，并在检索时更准确地定位到与用户提问相关的信息。

<h3 id="a3GWG">6.2.2、查询构建</h3>
**查询构建：**旨在通过查询增强的方式，扩展和丰富用户查询的语义和内容，提高检索结果的准确性和全面性，“钩”出相应的内容。增强方式可分为语义增强和内容增强。

+ **语义增强：**通过同义改写和多视角分解等方法来扩展、丰富用户查询的语义，以提高检索的准确性和全面性
+ **内容增强：**通过背景文档生成等方法挖掘出与查询主题相关的内容，提供更全面的知识支撑，丰富查询的广度和深度。    

<h3 id="ovN5O">6.2.3、文本检索</h3>
**文本检索：**旨在找到知识库中与用户查询相关的知识文本；检索效率增强旨在解决检索时性能瓶颈问题。

优化检索过程，提升检索的质量和效率，对改善RAG的性能具有重要意义。

检索的质量（召回率、精度、多样性等）会直接影响大模型的生成质量

检索的效率也是评估整个RAG系统性能的关键部分，极大影响用户的使用体验。

常见的文本检索器可以分为三类：判别式检索器、生成式检索器和图检索器

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742824342201-34f589d8-874e-4924-8471-01607bdd61c0.png)

<h4 id="TxkX6">6.2.3.1、判别式检索器</h4>
判别式检索器对问题和文档进行特征向量提取，以得到问题和文档的相关性分数。判别式检索器包括稀疏检索器，交叉检索器和双编码器三类。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742824465755-d78bba03-994b-4f16-9564-e802b0f8355c.png)

**稀疏检索器：**使用稀疏表示方法来匹配文本。其中TF-IDF是一种典型的稀疏检索方法。

+ TF-IDF：基于词频（TF）和逆文档频率（IDF）来衡量词语在语料库中的重要性，然后用此重要性对文本进行编码。
+ ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742884180877-712b60cf-b265-4eff-8a9a-201e418fe9ac.png)
+ ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742884284014-0e039bc0-36c2-40e8-ae7f-484ef5533708.png)

**双编码检索器（稠密检索器）：**首先将查询和文档首先各自通过独立的编码器生成各自的向量表示，再对这两个向量之间的相似度进行计算，以评估他们的相似性。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742884516627-4fe9375f-ed5b-4a22-906a-cc017504778d.png)

为了使双编码器检索器查询与文档在提取特征向量时缺乏交互问题，ColBERT以查询和文档间的Token级的相似度为度量，通过对比学习对双编码器进行微调。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742884644251-43315af0-a53f-4b9e-bf02-2473d60a4d19.png)

**交叉编码检索器”端到端“的给出查询与文档的相似度。**该类检索器将查询和文档结合后直接输入到模型中，最终模型输出一个介于0到1之间的数值，用以表示查询与文档之间的相似性。（一般先用稀疏编码器粗略的筛选，后续使用交叉编码器精排。）

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742884839957-fce72670-51ac-499d-a171-34d233c9117b.png)

**判别式检索器效率增强**

知识库文本中的向量编码存储在向量数据库中，对其编码进行逐一检索缓慢低效，高效的向量索引方法可以提高检索效率。

<h4 id="SQHiU">![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885167166-42661a34-ccef-4579-95a6-7fe9df22c675.png) 6.2.3.2、生成式检索器</h4>
生成式检索器直接将知识库中的文档信息记忆在模型参数中。然后，接收到查询请求时，能够直接生成相关文档的标识符（即Doc ID）,以完成检索。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885300099-6270559c-275e-47fe-b135-a5ab00e6dd1b.png)  ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885356152-ca18e0f8-b00b-4186-98fc-b0ad1cbe2d2f.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885406669-a5f908de-73f5-463b-b3c4-baae07adc900.png)

**生成式检索器效率增强**

生成式检索效率增强可以从多个关键方面入手，包括提升文档标识符的构建效率、加速增量更新过程以及优化推理环节等。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885565420-a4057ba5-a852-4930-8bfa-57ab9339d5c3.png)

<h4 id="TSGC4"> 6.2.3.3、图检索器</h4>
![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885677769-d25d409e-44c8-49a3-8afc-6e808e2e09f6.png)

图检索器的知识库为图数据库，包括开放知识图谱和自建图两种，它们一般由<主体、谓词和客体>三元组构成。这样做不仅可以捕捉概念之间的语义关系，还允许人类和机器人可以共同对知识进行理解和推理。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742885884037-22d6c384-e860-4ea0-8e1d-dc23cafff9ba.png)

基于图数据库的检索增强生成方法，能够改善在推理复杂信息时的问答性能。其核心链路可以分成图索引构建、图检索和生成三阶段。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886066082-e632639f-c109-4c35-a9f4-7eb5ed37c4ae.png)

**图检索效率增强**

图检索效率增强可以从多个方面入手，包括索引构建优化，检索优化等

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886178506-5d9d7b01-f044-42ea-95ca-a3b5c1fd0bcd.png)

<h3 id="U87s3">6.2.4、检索结果重排</h3>
检索阶段为了保证检索速度通常会损失一定的性能，可以检索到质量较低的文档。重排的目的是对检索到的段落进行进一步的排序精选。重排可以分为基于交叉编码的方法和基于上下文学习的方法。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886386998-56c69ee5-f130-4bcb-af9b-a41ecd2b7f27.png)

基于交叉编码的重排方法利用交叉编码器来评估文档与查询之间的语义相关性。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886476599-7a116c13-fc5c-4218-b64d-e966d3388848.png)

基于上下文学习的方法是指设计精巧的Prompt，使用大模型来执行重排任务

<h2 id="MCqSu">6.3、生成增强</h2>
检索器得到相关信息后，需要将其传递给大模型以增强模型的生成能力。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886720919-5ef41861-5242-4b9b-91b6-bd8986875047.png)

利用检索到的信息进行生成增强是一个复杂的过程，不同的方式会显著影响RAG的性能。可从三个角度进行分析：何时增强、何处增强、多次增强。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742886817359-85d02b21-59de-42a2-b16c-1eab67e5bc29.png)

<h3 id="ZvaLr">6.3.1、何时增强</h3>
判断是否需要增强的核心在于判断大模型是否具有内部知识。对于内部知识可以解决的问题，我们可以不对该问题进行增强。

+ **外部观测法：**不侵入模型内部参数。通过对大模型的训练数据和输出进行观测来判断大模型是否具备相应知识。（类似于面试考察应聘者）
+ **内部观测法：**在模型参数可以访问的情况下，观测模型内部的隐藏状态来更加精确的评估其知识掌握情况。（类似于对人体内部进行脑电波诊断）

<h4 id="LYBIk">6.3.1.1、外部观测法</h4>
**Prompt询问：**通过Prompt直接询问是否具备内部知识进行预测，根据模型的回答情况进行判断。局限性：模型过度自信。

**训练数据观测：**训练数据是大模型的知识来源，可以通过观察训练数据预测模型的内部知识水平。局限性：部分模型无法获取训练数据。

+ 知识在训练数据中的出现频率与模型对该知识的记忆程度是正相关的。
+ 根据知识在训练数据中的出现频率估计模型的学习情况。

**伪训练数据：**无法观察训练数据时，可以通过构造伪训练数据来拟合训练数据的相关情况。通过构造伪训练数据统计量对是否具备内部知识进行预测。

+ 流行度：实体或知识在特定环境中被广泛关注或使用的频率。
    - 由于模型对低频的知识掌握不足，而对更”流行“（高频）的知识掌握更好，因此流行度可以作为伪训练数据的统计量。
+ 流行度阈值：通过设定流行度阈值来判别模型是否具备相应的内部知识；
    - 流行度阈值可以设定为维基百科/谷歌搜索的某个访问量值或者热度值。
    - ![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742905388785-c4d36bd8-8fb3-4a10-bca7-c7c3076b3541.png)

<h4 id="ReLaU">6.3.1.2、内部观测法</h4>
通过分析大模型在生成文本时的内部状态变化，来评估其内部知识水平。

+ 在处理包含或不包含内部知识的不同问题时，模型的中间层（注意力层、MLP层）会展现出不同的动态变化。
+ 基于这一特性，我们可以训练分类器进行判别，这种方法称为探针。

对于输入问题，利用训练好的探针，即线性分类器，根据问题所对应的内部表示预测该问题是属于模型已知还是未知。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742906725983-048b229f-8630-4ef9-8b11-a5ba52d89f13.png)

<h3 id="S9BPA">6.3.2、何处增强</h3>
确定在模型中的何处融入检索到的知识以最大化效用

    - 在确定大模型需要外部知识后，我们需要考虑在何处利用检索到的外部知识，即何处增强的问题。
    - 得益于大模型的上下文学习能力、注意力机制的可扩展性以及自回归生成能力，其输入端、中间层和输出层都可以进行知识融合的操作。
+ **输入端：**将问题和检索到的外部知识拼接到Prompt中，然后输入给大模型；
+ **中间层：**采用交叉注意力将外部知识直接编码到模型的隐藏状态中；
+ **输出端：**利用外部知识对生成的文本进行矫正。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742908492965-b0e31b1a-e7cb-45e5-8f46-b5f1fdd4b17c.png)

<h4 id="RxTcs">6.3.2.1、在输入端增强</h4>
在输入端增强的方法直接将检索到的外部知识文本与用户查询拼接到Prompt中，然后输入给大模型，是当前主流的增强方法。

<h4 id="VErIS">6.3.2.2、在中间层增强</h4>
在中间层增强的方法先将检索到的外部知识转换为向量表示，然后将这些向量插入通过交叉注意力融合到模型的隐藏状态中。

代表性方法：RETRO，其在语言模型的中间层通过交叉编码将检索信息动态融合到模型的隐藏层中，从而增强模型的语义理解和生成能力。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742955627717-8a56579f-715f-4310-a010-6805b380808e.png)

<h4 id="hiIfy">6.3.2.3、在输出端增强</h4>
在输出端增强的方法利用检索到的外部知识对大模型生成的文本进行校准，是一种后处理的方法。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742955782982-6f779f76-ebf7-4219-b8c0-3dc6016fdd82.png)

 优点及缺点

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742955874387-5d42702c-23c2-4031-bb1a-69fba35562fc.png)

<h3 id="Uo0CC">6.3.3、多次增强</h3>
对复杂查询和模糊查询进行多次迭代增强，以提升RAG在困难问题上的效果。

复杂问题往往涉及多个知识点，需要多跳（multi-hop）的理解；而模糊问题往往指代范围不明，难以理解问题的含义。

<h4 id="RLeSB">6.3.3.1、多次迭代检索增强</h4>
对于复杂问题和模糊问题，我们难以通过以此检索增强就确保生成正确，多次迭代检索增强在所难免。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742956298665-742695e6-c9b8-4178-b4b6-de8576bf49fb.png)

<h4 id="qUwdv">6.3.3.2、分解式增强</h4>
在复杂问题的检索增强中，通常无法仅通过一次检索增强就得到满意的答案。可以采用分解式增强，将复杂问题化为多个子问题，在子问题间进行迭代检索增强 。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742956471997-6370eb33-7f0d-4908-966f-359d74461279.png)

<h4 id="gTn8b">6.3.3.3、细化式增强</h4>
在模糊问题中，问题的主题通常指代不明，容易引发歧义。可以通过细化式检索来引导大模型探索模糊问题的多种细化路径。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742956577458-17952565-0170-460a-b0c4-9147e5081081.png)

<h2 id="L1Zkc">6.4、降本增效</h2>
检索的外部知识通常包括大量文本，因此需要降本增效，以提升处理文本的效率。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742956659854-ffbd4b52-096e-4b2a-8cdf-347167bc9625.png)

检索出的外部知识通常包含大量原始文本。因此可以从去除文本与复用计算结果两个角度减少推理计算成本。

<h3 id="JSh78">6.4.1去除冗余文本</h3>
去除冗余文本的方法通过对检索出的原始文本的词句进行过滤，从中选择出部分有益于增强生成的部分。

去除冗余文本的方法主要有三类：Token级别的方法，全文本级别的方法以及子文本级别的方法。

**Token级别的方法**

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957471832-0f4e6739-23f6-491c-803f-09b1f36cfd67.png)

**全文本级别的方法**

全文本级别的方法直接从整个文档中抽取出重要信息，以去除冗余信息。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957536567-f37066c6-f4f9-404b-80d2-8c9a3919b17a.png)

**子文本级别的方法**

通过对文档分割后的子文本打分，删除掉不必要的子文本。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957626261-2daec9cb-862c-4d0b-b7e2-2333bfd938f0.png)

FIT-RAG的子文档压缩方法通过将文档划分为子文档，并利用预先构建的打分器对子文本进行打分，筛选出既包含事实信息又符合大模型偏好的少量子文本组合。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957746693-9dd11aa1-4e94-408d-a6e9-11a69a3ea627.png)

<h3 id="wzFLe">6.4.2、复用计算结果</h3>
除了对冗余信息进行筛出，还可以对计算必需的中间结果进行复用，以优化RAG效率。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957893723-94d1bc65-9ae5-42ea-a3d6-2e7b5f276923.png)

为避免每次生成新的token时重复计算之前的Key和Vaule，可以将它们缓存起来（KV-Cache），在需要时直接调用缓存结果，从而减少计算。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742957991475-329cd448-e92e-4cec-bc52-1e7a0089704a.png)

RAGCache是一种经典的方法，它设计了一种RAG系统专用的多级动态缓存机制，由三个核心部分组成：KV张量缓存库、缓存检索器和RAG控制器。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742958099930-df7dfeb3-203c-4f0c-998e-abe58f9f0c24.png)

<h2 id="K684z">6.5、RAG应用</h2>
<h3 id="V0tT0">6.5.1、多模态方面应用</h3>
在垂直领域中，除文本数据之外，对多模态数据的处理需求也日益凸显。

在医疗方面多模态数据十分普遍，RAG可以帮助融合不同模态的数据。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742958327777-42a27a42-7d2d-4fcc-b0b3-0e94d152b9f4.png)

<h3 id="lH9mg">6.5.2、Agent方面应用</h3>
在Agent系统运行中，需要检索并整合多样化信息资源，以准确的满足用户需求，RAG技术扮演者重要角色。

![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742958484202-4124462c-6270-4737-b6f4-ce0e1ec62945.png)

<h3 id="o2kdm">6.5.3、RAG企业落地</h3>
 在AI应用的未来发展中，RAG仍将作为核心技术引领AI产品的进步。![](https://cdn.nlark.com/yuque/0/2025/png/47022241/1742958548703-efcf9328-ef58-49d6-9c70-c8a28e6609e0.png)



